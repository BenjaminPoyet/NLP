{"metadata":{"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install nltk","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting nltk\n  Downloading nltk-3.5.zip (1.4 MB)\n\u001b[K     |████████████████████████████████| 1.4 MB 3.5 MB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: click in /srv/conda/envs/notebook/lib/python3.7/site-packages (from nltk) (7.1.1)\nRequirement already satisfied: joblib in /srv/conda/envs/notebook/lib/python3.7/site-packages (from nltk) (0.14.1)\nCollecting regex\n  Downloading regex-2020.4.4-cp37-cp37m-manylinux2010_x86_64.whl (679 kB)\n\u001b[K     |████████████████████████████████| 679 kB 25.2 MB/s eta 0:00:01\n\u001b[?25hCollecting tqdm\n  Downloading tqdm-4.45.0-py2.py3-none-any.whl (60 kB)\n\u001b[K     |████████████████████████████████| 60 kB 9.2 MB/s  eta 0:00:01\n\u001b[?25hBuilding wheels for collected packages: nltk\n  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for nltk: filename=nltk-3.5-py3-none-any.whl size=1434676 sha256=ddc035caf87870cce39a59442348e4429312ebea8cc3918a26ce877e80085777\n  Stored in directory: /home/jovyan/.cache/pip/wheels/45/6c/46/a1865e7ba706b3817f5d1b2ff7ce8996aabdd0d03d47ba0266\nSuccessfully built nltk\nInstalling collected packages: regex, tqdm, nltk\nSuccessfully installed nltk-3.5 regex-2020.4.4 tqdm-4.45.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import nltk\nnltk.download()","metadata":{"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"NLTK Downloader\n---------------------------------------------------------------------------\n    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n---------------------------------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Downloader>  d\n"},{"name":"stdout","text":"\nDownload which package (l=list; x=cancel)?\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  Identifier>  l\n"},{"name":"stdout","text":"Packages:\n  [ ] abc................. Australian Broadcasting Commission 2006\n  [ ] alpino.............. Alpino Dutch Treebank\n  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n  [ ] basque_grammars..... Grammars for Basque\n  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n                           Extraction Systems in Biology)\n  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n  [ ] book_grammars....... Grammars from NLTK Book\n  [ ] brown............... Brown Corpus\n  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n  [ ] cess_cat............ CESS-CAT Treebank\n  [ ] cess_esp............ CESS-ESP Treebank\n  [ ] chat80.............. Chat-80 Data Files\n  [ ] city_database....... City Database\n  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n  [ ] comparative_sentences Comparative Sentence Dataset\n  [ ] comtrans............ ComTrans Corpus Sample\n  [ ] conll2000........... CONLL 2000 Chunking Corpus\n  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Hit Enter to continue:  \n"},{"name":"stdout","text":"  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n                           and Basque Subset)\n  [ ] crubadan............ Crubadan Corpus\n  [ ] dependency_treebank. Dependency Parsed Treebank\n  [ ] dolch............... Dolch Word List\n  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n                           Corpus\n  [ ] floresta............ Portuguese Treebank\n  [ ] framenet_v15........ FrameNet 1.5\n  [ ] framenet_v17........ FrameNet 1.7\n  [ ] gazetteers.......... Gazeteer Lists\n  [ ] genesis............. Genesis Corpus\n  [ ] gutenberg........... Project Gutenberg Selections\n  [ ] ieer................ NIST IE-ER DATA SAMPLE\n  [ ] inaugural........... C-Span Inaugural Address Corpus\n  [ ] indian.............. Indian Language POS-Tagged Corpus\n  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n                           ChaSen format)\n  [ ] kimmo............... PC-KIMMO Data Files\n  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n  [ ] large_grammars...... Large context-free and feature-based grammars\n                           for parser comparison\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Hit Enter to continue:  \n"},{"name":"stdout","text":"  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n                           part-of-speech tags\n  [ ] machado............. Machado de Assis -- Obra Completa\n  [ ] masc_tagged......... MASC Tagged Corpus\n  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n  [ ] moses_sample........ Moses Sample Models\n  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n                           2015) subset of the Paraphrase Database.\n  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n  [ ] nombank.1.0......... NomBank Corpus 1.0\n  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n  [ ] nps_chat............ NPS Chat\n  [ ] omw................. Open Multilingual Wordnet\n  [ ] opinion_lexicon..... Opinion Lexicon\n  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n  [ ] paradigms........... Paradigm Corpus\n  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n                           Evaluation Shared Task\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Hit Enter to continue:  \n"},{"name":"stdout","text":"  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n                           character properties in Perl\n  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n  [ ] pl196x.............. Polish language of the XX century sixties\n  [ ] porter_test......... Porter Stemmer Test Files\n  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n  [ ] problem_reports..... Problem Report Corpus\n  [ ] product_reviews_1... Product Reviews (5 Products)\n  [ ] product_reviews_2... Product Reviews (9 Products)\n  [ ] propbank............ Proposition Bank Corpus 1.0\n  [ ] pros_cons........... Pros and Cons\n  [ ] ptb................. Penn Treebank\n  [ ] punkt............... Punkt Tokenizer Models\n  [ ] qc.................. Experimental Data for Question Classification\n  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n                           version\n  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n                           Portuguesa)\n  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n  [ ] sample_grammars..... Sample Grammars\n  [ ] semcor.............. SemCor 3.0\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Hit Enter to continue:  \n"},{"name":"stdout","text":"  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n  [ ] sentiwordnet........ SentiWordNet\n  [ ] shakespeare......... Shakespeare XML Corpus Sample\n  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n  [ ] smultron............ SMULTRON Corpus Sample\n  [ ] snowball_data....... Snowball Data\n  [ ] spanish_grammars.... Grammars for Spanish\n  [ ] state_union......... C-Span State of the Union Address Corpus\n  [ ] stopwords........... Stopwords Corpus\n  [ ] subjectivity........ Subjectivity Dataset v1.0\n  [ ] swadesh............. Swadesh Wordlists\n  [ ] switchboard......... Switchboard Corpus Sample\n  [ ] tagsets............. Help on Tagsets\n  [ ] timit............... TIMIT Corpus Sample\n  [ ] toolbox............. Toolbox Sample Files\n  [ ] treebank............ Penn Treebank Sample\n  [ ] twitter_samples..... Twitter Samples\n  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n                           (Unicode Version)\n  [ ] udhr................ Universal Declaration of Human Rights Corpus\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Hit Enter to continue:  \n"},{"name":"stdout","text":"  [ ] unicode_samples..... Unicode Samples\n  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n  [ ] vader_lexicon....... VADER Sentiment Lexicon\n  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n  [ ] webtext............. Web Text Corpus\n  [ ] wmt15_eval.......... Evaluation data from WMT15\n  [ ] word2vec_sample..... Word2Vec Sample\n  [ ] wordnet............. WordNet\n  [ ] wordnet_ic.......... WordNet-InfoContent\n  [ ] words............... Word Lists\n  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n                           English Prose\n\nCollections:\n  [ ] all-corpora......... All the corpora\n  [ ] all-nltk............ All packages available on nltk_data gh-pages\n                           branch\n  [ ] all................. All packages\n  [ ] book................ Everything used in the NLTK Book\n  [ ] popular............. Popular packages\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Hit Enter to continue:  \n"},{"name":"stdout","text":"  [ ] tests............... Packages for running tests\n  [ ] third-party......... Third-party data packages\n\n([*] marks installed packages)\n\nDownload which package (l=list; x=cancel)?\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  Identifier>  x\n"},{"name":"stdout","text":"\n---------------------------------------------------------------------------\n    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n---------------------------------------------------------------------------\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Downloader>  q\n"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"nltk.download('names')","metadata":{"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package names to /home/jovyan/nltk_data...\n[nltk_data]   Unzipping corpora/names.zip.\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"## Prediction","metadata":{}},{"cell_type":"code","source":"# Load data and training \nnames = ([(name, 'male') for name in names.words('male.txt')] + \n         [(name, 'female') for name in names.words('female.txt')])","metadata":{"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#Feature extraction: Based on the dataset, we prepare our feature. The feature we will use is the last letter of a name:\n#We define a featureset using:\ndef gender_features(word): \n    return {'last_letter': word[-1]} \n\nfeaturesets = [(gender_features(n), g) for (n,g) in names] \ntrain_set = featuresets","metadata":{"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"classifier = nltk.NaiveBayesClassifier.train(train_set) \n\n# Predict\nprint('Frank is a', classifier.classify(gender_features('Frank')),\"name\")\nprint('Ben is a', classifier.classify(gender_features('Ben')),\"name\")\nprint('Claire is a', classifier.classify(gender_features('Claire')),\"name\")\nprint('Rosa is a', classifier.classify(gender_features('Rosa')),\"name\")","metadata":{"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Frank is a male name\nBen is a male name\nClaire is a female name\nRosa is a female name\n","output_type":"stream"}]},{"cell_type":"code","source":"# Predict with input from user\nname = input(\"Name: \")\nprint(classifier.classify(gender_features(name)))","metadata":{"trusted":true},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdin","text":"Name:  Benji\n"},{"name":"stdout","text":"female\n","output_type":"stream"}]},{"cell_type":"code","source":"#ouch","metadata":{"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Sentiment Analysis\n\nIn Natural Language Processing there is a concept known as Sentiment Analysis.\nGiven a movie review or a tweet, it can be automatically classified in categories.\nThese categories can be user defined (positive, negative) or whichever classes you want.","metadata":{}},{"cell_type":"code","source":"#We start by defining 3 classes: positive, negative and neutral.\npositive_vocab = [ 'awesome', 'outstanding', 'fantastic', 'terrific', 'good', 'nice', 'great', ':)' ]\nnegative_vocab = [ 'bad', 'terrible','useless', 'hate', ':(' ]\nneutral_vocab = [ 'movie','the','sound','was','is','actors','did','know','words','not' ]","metadata":{"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"#Every word is converted into a feature using a simplified bag of words model:\n\ndef word_feats(words):\n    return dict([(word, True) for word in words])\n\npositive_features = [(word_feats(pos), 'pos') for pos in positive_vocab]\nnegative_features = [(word_feats(neg), 'neg') for neg in negative_vocab]\nneutral_features = [(word_feats(neu), 'neu') for neu in neutral_vocab]","metadata":{"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#defining the training set as the sum of these features\ntrain_set = negative_features + positive_features + neutral_features","metadata":{"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"classifier = nltk.NaiveBayesClassifier.train(train_set) ","metadata":{"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Predict\nneg = 0\npos = 0\nsentence = \"Awesome movie, I liked it\"\nsentence = sentence.lower()\nwords = sentence.split(' ')\nfor word in words:\n    classResult = classifier.classify( word_feats(word))\n    if classResult == 'neg':\n        neg = neg + 1\n    if classResult == 'pos':\n        pos = pos + 1\n\nprint('Positive: ' + str(float(pos)/len(words)))\nprint('Negative: ' + str(float(neg)/len(words)))","metadata":{"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Positive: 0.6\nNegative: 0.2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We can apply this model to the following dataset  \nmovie sentiment: https://www.cs.cornell.edu/people/pabo/movie-review-data/  \nhttps://github.com/nltk/nltk/wiki/Sentiment-Analysis\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}